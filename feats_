import pandas as pd
import numpy as np
import numpy
import pandas
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from fracdiff.sklearn import Fracdiff
import numba
import time
import concurrent.futures
import traceback
sns.set_style("darkgrid")

# ===================  raw prado frac diff function ======================================================
def getWeights(d, size):
    # _thresh > 0 drops insignificant weights
    w = [1.]
    for k in range(1, size):
        w_ = -w[-1]/ k*(d-k+1)
        w.append(w_)
    w = numpy.array(w[::-1]).reshape(-1,1)
    return w
def plotWeights(dRange, nPlots, size):
    w = pandas.DataFrame()
    for d in numpy.linspace(dRange[0], dRange[1], nPlots):
        w_ = getWeights(d, size=size)
        w_ = pandas.DataFrame(w_, index=range(w_.shape[0])[::-1], columns=[d])
        w = w.join(w_, how='outer')
    # ax = w.plot()
    # ax.legend(loc='upper left'); plt.show()
    return
def fracDiff(series, d, _thresh=0.1):
    '''
    Increasing width window, with treatment of NaNs
    Note 1: For _thresh=1, nothing is skipped
    Note 2: d can be any positive fractional, not necessarily bounded [0,1]
    '''
    #1) Compute weights for the longest series
    w = getWeights(d, series.shape[0])
    #2) Determine initial calc to be skipped based on weight-loss _threshhold
    w_ = numpy.cumsum(abs(w))
    w_ /= w_[-1]
    skip = w_[w_ > _thresh].shape[0]
    #3) Apply weights to values
    df = {}
    for name in series.columns:
        seriesF, df_ = series[[name]].fillna(method='ffill').dropna(), pandas.Series()
        for iloc in range(skip, seriesF.shape[0]):
            loc = seriesF.index[iloc]
            if not numpy.isfinite(series.loc[loc,name]): continue # exclude NAs
            df_[loc] = numpy.dot(w[-(iloc+1):,:].T, seriesF.loc[:loc])[0,0]
        df[name] = df_.copy(deep=True)
    df = pandas.concat(df, axis=1)
    return df
def getWeights_FFD(d, _thresh):
    # _thresh>0 drops insignificant weights
    w, k = [1.], 1
    while True:
        w_ = -w[-1]/k * (d-k+1)
        if abs(w_) < _thresh:
            break
        w.append(w_)
        k += 1
    w = numpy.array(w[::-1]).reshape(-1,1)
    return w
def fracDiff_FFD(series, d, _thresh=0.01):
    '''
    Constant width window (new solution)
    Note 1: _thresh determines the cut-off weight for the window
    Note 2: d can be any positive fractional, not necessarily bounded [0,1].
    '''
    #1) Compute weights for the longest series
    w = getWeights_FFD(d, _thresh)
    width = len(w) - 1
    #2) Apply weights to values
    df = {}
    for name in series.columns:
        seriesF = series[[name]].fillna(method='ffill').dropna()
        df_ = pandas.Series()

        for iloc1 in range(width, seriesF.shape[0]):
            loc0, loc1 = seriesF.index[iloc1-width], seriesF.index[iloc1]
            if not numpy.isfinite(series.loc[loc1,name]):
                continue # exclude NAs
            df_[loc1] = numpy.dot(w.T, seriesF.loc[loc0:loc1])[0,0]
        df[name] = df_.copy(deep=True)
    df = pandas.concat(df, axis=1)
    return df
def plotMinFFD(df, col_name='close'):
    from statsmodels.tsa.stattools import adfuller
    out = pandas.DataFrame(columns=['adfStat', 'pVal', 'lags', 'nObs', '95% conf', 'corr'])
    df0 = df.copy(deep=True)

    # Shift the series to handle negative values
    shift_val = 0
    if df0[col_name].min() <= 0:
        shift_val = abs(df0[col_name].min()) + 1
        df0[col_name] = df0[col_name] + shift_val

    for d in numpy.linspace(0,1,11):
        df1 = numpy.log(df0[[col_name]]).resample('1D').last() # downcast to daily obs
        df2 = fracDiff_FFD(df1, d, _thresh=0.1)
        corr = numpy.corrcoef(df1.loc[df2.index, col_name], df2[col_name])[0,1]
        df2 = adfuller(df2[col_name], maxlag=1, regression='c', autolag=None)
        out.loc[d] = list(df2[:4]) + [df2[4]['5%']] + [corr] # with critical value

    out[['adfStat', 'corr']].plot(secondary_y='adfStat')
    plt.axhline(out['95% conf'].mean(), linewidth=1, color='r', linestyle='dotted')
    plt.title(f'{col_name.upper()} Min-Max Frac Differencing: Correlation vs. D')
    plt.show()
    return out
def loop_dval_csv(file_path, save_path, columns, d_range, _thresh,log=False):
    # Read the data
    data = pd.read_csv(file_path, parse_dates=['date'])  # Parse 'date' column as datetime
    data.set_index('date', inplace=True)  # Set 'date' column as the index

    result_df = pd.DataFrame(index=data.index)
    if log:
        data = numpy.log(data) #todo log data # logged all data dont use


    # Loop through the columns and d values
    for col in columns:
        for d in d_range:
            ffd_data = fracDiff_FFD(data[[col]], d, _thresh)
            result_df[f'{col}_d{d}'] = ffd_data.squeeze()  # squeeze() to convert DataFrame to Series

    # Add 100 to each column
    # result_df += 100

    # Only log columns
    # result_df = np.log(result_df)
    ## log? as in ln(10)
    #

    # Save the result to a CSV file
    result_df.to_csv(save_path)
def apply_fracDiff_FFD(df, _dval, _thresh):
    df_fracdiff = pd.DataFrame(index=df.index)
    for col in df.columns:
        ffd_data = fracDiff_FFD(df[[col]], _dval, _thresh)
        df_fracdiff[f"{col}_ffd"] = ffd_data.squeeze()
    return df_fracdiff


# ===================  statsmodels frac diff import =====================================================
def apply_fracdiff(df, _dval):
    fd = Fracdiff(_dval)
    fracdiff_df = pd.DataFrame(index=df.index)
    for col in df.columns:
        ticker = col.split('_')[0]  # Extract the ticker name
        _nmonic = '_'.join(col.split('_')[1:])  # Extract the _nmonic
        fd.fit(df[col].values.reshape(-1, 1))  # Fitting the model
        fracdiff_series = fd.transform(df[col].values.reshape(-1, 1)).flatten()
        # Use the same column naming convention as loop_dval_csv_optimized_parallel
        fracdiff_df[f"{ticker}_{_nmonic}_d{_dval}"] = pd.Series(fracdiff_series, index=df.index)
    return fracdiff_df

# ===================  optimized prado frac d via claude ================================================
@numba.jit(nopython=True)
def get_weights_ffd(d, _thresh):
    w = [1.]
    k = 1
    while True:
        w_ = -w[-1] / k * (d - k + 1)
        if abs(w_) < _thresh:
            break
        w.append(w_)
        k += 1
    return np.array(w[::-1])
@numba.jit(nopython=True)
def frac_diff_ffd_no_padding(x, d, _thresh):
    w = get_weights_ffd(d, _thresh)
    width = len(w) - 1
    output = np.empty(len(x) - width)
    for i in range(width, len(x)):
        output[i - width] = np.dot(w, x[i - width:i + 1])
    return output
def fracDiff_FFD_optimized(series, d, _thresh=0.01):
    series = series.fillna(method='ffill').dropna()
    columns = series.columns
    index = series.index[len(get_weights_ffd(d, _thresh)) - 1:]

    result = np.empty((len(index), len(columns)))
    for i, col in enumerate(columns):
        result[:, i] = frac_diff_ffd_no_padding(series[col].values, d, _thresh)

    return pd.DataFrame(result, index=index, columns=columns)
def process_column(data, col, d_range, _thresh):
    result = {}
    for d in d_range:
        ffd_data = fracDiff_FFD_optimized(data[[col]], d, _thresh)
        result[f'{col}_d{d}'] = ffd_data
    return result
def loop_dval_csv_optimized_parallel(data, columns, d_range, _thresh, _log=False, max_workers=None):
    if _log:
        data = np.log(data)

    result_df = pd.DataFrame(index=data.index)

    #

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_column, data, col, d_range, _thresh) for col in columns]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            for key, value in result.items():
                result_df[key] = value

    return result_df


# ===================  plt function with bollinger  ===================================================
def plot_fracdiff_bollinger(df_fracdiff, _lookbck, _stdv):
    sns.set_style("darkgrid", {"grid.color": "white"})
    fig, axs = plt.subplots(len(df_fracdiff.columns), 1, figsize=(14, 17))

    if not isinstance(axs, np.ndarray):
        axs = [axs]

    for i, col in enumerate(df_fracdiff.columns):
        df_fracdiff['SMA'] = df_fracdiff[col].rolling(window=_lookbck).mean()
        df_fracdiff['STD'] = df_fracdiff[col].rolling(window=_lookbck).std()
        df_fracdiff['Upper_BB'] = df_fracdiff['SMA'] + (_stdv * df_fracdiff['STD'])
        df_fracdiff['Lower_BB'] = df_fracdiff['SMA'] - (_stdv * df_fracdiff['STD'])

        plot_df = df_fracdiff.dropna()

        sns.lineplot(x=plot_df.index, y=plot_df[col], ax=axs[i], label=col,
                     linewidth=0.8, alpha=0.9, color='darkblue')
        sns.lineplot(x=plot_df.index, y=plot_df['Upper_BB'], ax=axs[i], color='darkgreen',
                     linewidth=0.7, alpha=0.9)
        sns.lineplot(x=plot_df.index, y=plot_df['Lower_BB'], ax=axs[i], color='darkgreen',
                     linewidth=0.7, alpha=0.9)
        axs[i].fill_between(plot_df.index, plot_df['Lower_BB'], plot_df['Upper_BB'], color='lightgreen',
                            alpha=0.1)

        latest_value = plot_df[col].iloc[-1]
        axs[i].axhline(latest_value, color='red', linewidth='0.5', linestyle='--',
                       label=f'Last: {latest_value:.2f}')

        axs[i].grid(True, linewidth=1.75)
        axs[i].set_title(f"{col} - Bollinger Bands (Lookback: {_lookbck}, Std: {_stdv})")
        axs[i].set_xlabel("")
        axs[i].legend()
        if i != len(df_fracdiff.columns) - 1:
            axs[i].set_xticklabels([])

    plt.tight_layout()
    plt.subplots_adjust(hspace=0.02)
    plt.show()

def plt_variants(df, ticker, _nmonic, _dval, _thresh):
    # Prepare data
    df_ticker = df[[f"{ticker}_{_nmonic}"]].copy()

    try:
        # Seasonal adjustments
        df_sa_raw = seas_adj_indx(df_ticker)
        df_sa_decomp = seas_adj_decomp(df_ticker)
        col_name = f"{ticker}_{_nmonic}_sa"
        df_sa_raw = df_sa_raw[[col_name]]
        df_sa_decomp = df_sa_decomp[[col_name]] if col_name in df_sa_decomp.columns else df_sa_raw

        # Fractional differencing
        options = {
            'prado_raw_seas_raw': apply_fracDiff_FFD(df_sa_raw, _dval, _thresh),
            'prado_raw_seas_decomp': apply_fracDiff_FFD(df_sa_decomp, _dval, _thresh),
            'statsmdl_seas_raw': apply_fracdiff(df_sa_raw, _dval),
            'statsmdl_seas_decomp': apply_fracdiff(df_sa_decomp, _dval),
            'prado_fast_seas_raw': loop_dval_csv_optimized_parallel(df_sa_raw, df_sa_raw.columns, [_dval], _thresh),
            'prado_fast_seas_decomp': loop_dval_csv_optimized_parallel(df_sa_decomp, df_sa_decomp.columns, [_dval], _thresh)
        }

        # Plotting
        plt.figure(figsize=(16, 10))
        for name, data in options.items():
            linestyle = '--' if 'decomp' in name else '-'
            linestyle = ':' if 'fast' in name else linestyle
            plt.plot(data.index, data.iloc[:, 0], label=name, linewidth=0.5, alpha=0.9, linestyle=linestyle)

        plt.title(f"Comparison of Fractional Differencing Methods for {ticker.upper()}_{_nmonic}", fontsize=16)
        plt.xlabel("Date")
        plt.ylabel("Value")
        plt.legend(loc='upper left', fontsize='large')
        plt.grid(True)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        print("Please check your data and ensure it's in the correct format.")

def plt_variants2(df, ticker, _nmonic):
    df_ticker = df[[f"{ticker}_{_nmonic}"]].copy()
    column_name = f"{ticker}_{_nmonic}"

    options = {
        'raw_no_cv': {'method': 'mean', 'cv': False, 'yrs_indx': None},
        'raw_cv': {'method': 'mean', 'cv': True, 'yrs_indx': None},
        'indx_abs_no_cv': {'method': 'mean', 'cv': False, 'yrs_indx': 'abs'},
        'indx_abs_cv': {'method': 'mean', 'cv': True, 'yrs_indx': 'abs'}
    }

    results = {}
    for name, params in options.items():
        try:
            print(f"\nProcessing {name}")
            seasonal_index = bld_seas_indx_cstm_cv(df_ticker, column_name, **params)

            df_ticker['month_day'] = df_ticker.index.strftime('%m-%d')
            df_ticker['year'] = df_ticker.index.year

            # Apply the seasonal adjustment
            if params['yrs_indx'] == 'abs':
                adjusted = df_ticker.apply(lambda row: row[column_name] - seasonal_index.loc[row['month_day'], row['year']], axis=1)
            else:
                adjusted = df_ticker.apply(lambda row: row[column_name] - seasonal_index.loc[row['month_day'], row['year']], axis=1)

            results[name] = adjusted

            print(f"Shape of results[{name}]: {results[name].shape}")
            print(f"First few values of results[{name}]:\n{results[name].head()}")
        except Exception as e:
            print(f"Error processing {name}: {str(e)}")
            print(f"Traceback: {traceback.format_exc()}")
        print("---")

    plt.figure(figsize=(16, 10))
    for name, data in results.items():
        plt.plot(data.index, data, label=name, linewidth=0.8, alpha=0.7)

    plt.title(f"Comparison of Seasonal Adjustment Methods for {ticker.upper()}_{_nmonic}", fontsize=16)
    plt.xlabel("Date")
    plt.ylabel("Seasonally Adjusted Value")
    plt.legend(loc='upper left', fontsize='small')
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# =================== old seasonal fucntions===================================================================
def seas_adj_decomp(df):
    """
     pros: detrends data.
     cons: can cross validate and remove leakage. also has to be exact via resampling to 365 ex 366 days of year
     note: unlikely we will use this for short term data series where leakage is a concern

    """

    df_sa = pd.DataFrame(index=df.index)

    for col in df.columns:
        print(f"Processing column: {col}")
        print(f"Original data shape: {df[col].shape}")

        # Skip non-numeric columns
        if not np.issubdtype(df[col].dtype, np.number):
            print(f"Skipping non-numeric column: {col}")
            continue

        # Ensure we're working with numeric data
        numeric_data = pd.to_numeric(df[col], errors='coerce')
        print(f"After converting to numeric: {numeric_data.shape}")

        # Drop any NaN values that resulted from the conversion
        numeric_data = numeric_data.dropna()
        print(f"After dropping NaNs: {numeric_data.shape}")

        # Perform seasonal decomposition
        result = seasonal_decompose(numeric_data, model='additive', period=365)
        df_sa[f"{col}_sa"] = df[col] - result.seasonal

    return df_sa
def bld_seas_indx(df, column_name, method='mean'):
    """
    pros: theoretically we can add a version or custom version that excludes leakage via a cv method (that we need to build)
    cons: cant detrend data, only seasonalizes it.
    other: can add a nice feature such as olympic average or median
    other: can build an INDEX for each year, and then apply that index to the data. This may be best approach for some non-stationary data

    """

    df['month_day'] = df.index.strftime('%m-%d')  # Create a new column with month-day combination
    # Create a pivot table with month_day as rows and year as columns
    seasonal_df = df.pivot_table(values=column_name, index='month_day', columns=df.index.year, aggfunc='first')
    seasonal_df = seasonal_df.sort_index()  # Sort the index to ensure correct order (Jan-01 to Dec-31)
    seasonal_df = seasonal_df.fillna(method='ffill')  # Forward fill any remaining NaNs within each year

    current_year = df.index.max().year  # Calculate the average for all years except the current year
    historical_df = seasonal_df.drop(columns=[current_year])

    if method == 'median':
        seasonal_index = historical_df.median(axis=1)
    elif method == 'mean':
        seasonal_index = historical_df.mean(axis=1)
    elif method == 'olympic':
        # Olympic average: remove highest and lowest, then take mean
        seasonal_index = historical_df.apply(lambda x: x.sort_values()[1:-1].mean(), axis=1)
    else:
        raise ValueError("Method must be 'median', 'mean', or 'olympic'")

    if '02-29' not in seasonal_index.index:  # Ensure we have 366 days (including Feb 29 for leap years)
        feb_28_value = seasonal_index.loc['02-28']
        seasonal_index = pd.concat([seasonal_index, pd.Series({'02-29': feb_28_value})])
        seasonal_index = seasonal_index.sort_index()
    return seasonal_index
def seas_adj_indx(df):
    """
    converts seas_index to a vertical df or serries to add to the data

    """

    df_sa = pd.DataFrame(index=df.index)
    for col in df.columns:
        seasonal_index = bld_seas_indx(df, col)
        df['month_day'] = df.index.strftime('%m-%d')
        df_sa[f"{col}_sa"] = df[col] - df['month_day'].map(seasonal_index)
    return df_sa


# ===================load functions=======================================================================
def load_and_preprocess_data(file_path, tickers, _nmonic, _start_yr=2005):
    df = pd.read_csv(file_path)

    df.columns = [col.lower() for col in df.columns]
    df['date'] = pd.to_datetime(df['date'])

    df.set_index('date', inplace=True)
    df = df.sort_index()  # Ensure the index is sorted
    df = df.loc[~df.index.duplicated(keep='first')]

    # Create a complete date range
    date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')
    df = df.reindex(date_range)

    df = df.ffill()  # Forward fill without resampling

    # drop all feb 29ths
    df = df[~((df.index.month == 2) & (df.index.day == 29))]
    df = df[df.index.year >= _start_yr]

    selected_columns = [f"{ticker.lower()}_{_nmonic}" for ticker in tickers]
    return df[selected_columns]

# =================== new seasonal fucntions===================================================================
def bld_seas_indx_cstm_cv(df, column_name, method='mean', cv=True, yrs_indx=None):
    df = df.copy()
    df['month_day'] = df.index.strftime('%m-%d')
    df['year'] = df.index.year

    # Ensure the column is numeric
    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

    seasonal_df = df.pivot_table(values=column_name, index='month_day', columns='year', aggfunc='first')
    seasonal_df = seasonal_df.sort_index()
    seasonal_df = seasonal_df.fillna(method='ffill')

    # Identify complete years (those with all 365 days)
    complete_years = seasonal_df.columns[seasonal_df.count() == 365]
    all_years = seasonal_df.columns

    if cv:
        cv_indices = {}
        for year in all_years:
            other_years = [y for y in complete_years if y != year]
            if other_years:
                if method == 'median':
                    year_index = seasonal_df[other_years].median(axis=1)
                elif method == 'mean':
                    year_index = seasonal_df[other_years].mean(axis=1)
                elif method == 'olympic':
                    year_index = seasonal_df[other_years].apply(lambda x: x.sort_values()[1:-1].mean(), axis=1)
                else:
                    raise ValueError("Method must be 'median', 'mean', or 'olympic'")
                cv_indices[year] = year_index
        seasonal_index = pd.DataFrame(cv_indices)
    else:
        if method == 'median':
            seasonal_index = seasonal_df[complete_years].median(axis=1)
        elif method == 'mean':
            seasonal_index = seasonal_df[complete_years].mean(axis=1)
        elif method == 'olympic':
            seasonal_index = seasonal_df[complete_years].apply(lambda x: x.sort_values()[1:-1].mean(), axis=1)
        else:
            raise ValueError("Method must be 'median', 'mean', or 'olympic'")

        # For non-CV case, we create a DataFrame with the same seasonal index for all years
        seasonal_index = pd.DataFrame({year: seasonal_index for year in all_years})

    if yrs_indx:
        yearly_means = seasonal_index.mean()
        if yrs_indx == 'abs':
            seasonal_index = seasonal_index.sub(yearly_means, axis=1)
        elif yrs_indx == 'pct':
            seasonal_index = seasonal_index.div(yearly_means, axis=1) - 1
        else:
            raise ValueError("yrs_indx must be 'abs' or 'pct'")

    # Add February 29th if it's missing
    if '02-29' not in seasonal_index.index:
        feb_28_values = seasonal_index.loc['02-28']
        seasonal_index.loc['02-29'] = feb_28_values

    # Sort the index
    seasonal_index = seasonal_index.sort_index()

    return seasonal_index

def seas_adj_indx_cstm_cv(df, method='mean', cv=True, yrs_indx=None):
    df_sa = pd.DataFrame(index=df.index)
    for col in df.columns:
        seasonal_index = bld_seas_indx_cstm_cv(df, col, method, cv, yrs_indx)
        df['month_day'] = df.index.strftime('%m-%d')

        if cv and isinstance(seasonal_index, pd.DataFrame):
            # For CV case, we need to select the appropriate year's index for each row
            if yrs_indx == 'pct':
                df_sa[f"{col}_sa"] = df.apply(
                    lambda row: row[col] / (1 + seasonal_index.loc[row['month_day'], row.name.year]), axis=1)
            else:  # 'abs' or None
                df_sa[f"{col}_sa"] = df.apply(
                    lambda row: row[col] - seasonal_index.loc[row['month_day'], row.name.year], axis=1)
        else:
            # If seasonal_index is not a DataFrame or not CV, use it directly
            if yrs_indx == 'pct':
                df_sa[f"{col}_sa"] = df[col] / (1 + df['month_day'].map(seasonal_index))
            else:  # 'abs' or None
                df_sa[f"{col}_sa"] = df[col] - df['month_day'].map(seasonal_index)

    return df_sa

# ====================== bollingers and process=======================================================

def calculate_bollinger_bands(df, column, _lookbck, _stdv):
    sma = df[column].rolling(window=_lookbck).mean()
    std = df[column].rolling(window=_lookbck).std()
    lower_bb = sma - (_stdv * std)
    upper_bb = sma + (_stdv * std)
    return lower_bb, upper_bb

def process_tickers(df, df_sa, df_fracdiff, tickers, _nmonic, _dval, _lookbck, _stdv, yrs_indx, _plt_all=False, _plt_1st=True):
    result_df = pd.DataFrame()
    for ticker in tickers:
        raw_column = f"{ticker}_{_nmonic}"
        sa_column = f"{ticker}_{_nmonic}_sa"
        possible_fracdiff_columns = [
            f"{ticker}_{_nmonic}_sa_d{_dval}",
            f"{sa_column}_d{_dval}",
            f"{sa_column}_fracdiff"
        ]
        fracdiff_column = next((col for col in possible_fracdiff_columns if col in df_fracdiff.columns), None)

        if raw_column not in df.columns or sa_column not in df_sa.columns or fracdiff_column is None:
            print(f"Warning: Required columns for {ticker} not found. Skipping.")
            continue

        # Raw data
        result_df[f"{ticker}_{_nmonic}_raw"] = df[raw_column]

        # Seasonally adjusted data
        result_df[f"{ticker}_{_nmonic}_seas_adj_cv"] = df_sa[sa_column]

        # Seasonal component
        if yrs_indx == 'pct':
            result_df[f"{ticker}_{_nmonic}_seas_cv"] = df[raw_column] / df_sa[sa_column] - 1
        else:  # 'abs' or None
            result_df[f"{ticker}_{_nmonic}_seas_cv"] = df[raw_column] - df_sa[sa_column]

        # Fractionally differenced data
        result_df[f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}"] = df_fracdiff[fracdiff_column]

        # Bollinger Bands
        lower_bb, upper_bb = calculate_bollinger_bands(df_fracdiff, fracdiff_column, _lookbck, _stdv)
        result_df[f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}_bol_low_{_lookbck}_{_stdv}std"] = lower_bb
        result_df[f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}_bol_high_{_lookbck}_{_stdv}std"] = upper_bb

        # Difference between fractionally differenced data and Bollinger Bands
        result_df[f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}_bol_low_diff"] = df_fracdiff[fracdiff_column] - lower_bb
        result_df[f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}_bol_high_diff"] = upper_bb - df_fracdiff[fracdiff_column]

    if _plt_all:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 16))

        # First chart: Seasonal Component for All Tickers
        for ticker in tickers:
            seas_cv_column = f"{ticker}_{_nmonic}_seas_cv"
            sns.lineplot(x=result_df.index, y=result_df[seas_cv_column], ax=ax1, label=ticker, linewidth=0.8, alpha=0.9)
        ax1.set_title("Seasonal Component for All Tickers")
        ax1.legend()

        # Second chart: Correlation Matrix
        seas_cv_columns = [f"{ticker}_{_nmonic}_seas_cv" for ticker in tickers]
        corr_matrix = result_df[seas_cv_columns].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, ax=ax2)
        ax2.set_title("Correlation Matrix of Seasonal Components")
        ax2.set_xticklabels(tickers, rotation=45, ha='right')
        ax2.set_yticklabels(tickers, rotation=0)
        ax2.set_xlabel('')
        plt.tight_layout()
        plt.show()

    if _plt_1st:  # only plot the first ticker
        fig, axs = plt.subplots(3, 1, figsize=(14, 17), sharex=True)
        ticker = tickers[0]
        raw_column = f"{ticker}_{_nmonic}_raw"
        sa_column = f"{ticker}_{_nmonic}_seas_adj_cv"
        seas_cv_column = f"{ticker}_{_nmonic}_seas_cv"
        seas_adj_fracd_column = f"{ticker}_{_nmonic}_seas_adj_fracd_{_dval}"
        axs[0].plot(result_df.index, result_df[raw_column], label="Raw", linewidth=0.8, alpha=0.9)
        axs[0].plot(result_df.index, result_df[sa_column], label="Seas Adj", linewidth=0.8, alpha=0.9)
        sns.lineplot(x=result_df.index, y=result_df[seas_cv_column], ax=axs[1], label=seas_cv_column, linewidth=0.8,
                     alpha=0.9)
        sns.lineplot(x=result_df.index, y=result_df[seas_adj_fracd_column], ax=axs[2], label=seas_adj_fracd_column,
                     linewidth=0.8, alpha=0.9)
        axs[0].legend()
        axs[1].legend()
        axs[2].legend()
        axs[0].set_title(f"{ticker} - Raw and Seasonally Adjusted Data")
        axs[1].set_title(f"{ticker} - Seasonal Component")
        axs[2].set_title(f"{ticker} - Fractionally Differenced Data")
        axs[0].set_xlabel("")
        axs[1].set_xlabel("")
        plt.tight_layout()
        # plt.subplots_adjust(hspace=0.3)
        plt.show()

    return result_df

# ===================  main function =================================================================
def main():
    """
     ticker + suffiz = column name

     1. provides 2 functions of seasonal adjustment and fractional differencing
     2. applies these functions to the data via bollinger bands vs history

    """

    # input_file = r"C:\Optima Labs Dropbox\Omega Files\Omega_History_New.csv"
    input_file = r"C:\Optima Labs Dropbox\WAS\was_balance_sheet\_raps_.csv"

    tickers = [
            'cl',

            'c',
            's',
            'w',

            'lc',
            'lh',
            # 'fc', #not available? odd

            # 'cl',

    ]

    _nmonic = "rap"  #column= ticker__nmonic eg. c_rap
    _dval = 0.15 # 0.1, through 0.6 seem to work most often for us.
    _thresh = 0.01 #unless you are dealing with S&P 500 data, this is a good thresh with log=False
    _lookbck = 365  # You can adjust this
    _stdv = 1.75  # Number of standard deviations for Bollinger Bands

    df = load_and_preprocess_data(input_file, tickers, _nmonic, _start_yr=2005)
    # df['s_rap'] = df['s_rap'] + 500 # note: if value goes negative need to add a constant, s_rap is negative

    _yrs_index = next(index for index in [
        # 'abs',  # note if non stationary data, but has negatives
        'pct',  # note: if non stationary data, but has no negatives
        # None  # note: None is the default, no adjustment. if ur data has negatives or is very stationary
    ] if isinstance(index, str) or index is None)

    _method = next(method for method in [
        'mean',
        # 'median', #haven't had much luck on smaller datasets
        # 'olympic'  # throws out high and low. good to check to see how much things change... but cuts data size a lot
    ] if method)

    df_sa = seas_adj_indx_cstm_cv(df, method=_method, cv=True, yrs_indx=_yrs_index)
    #  cv=True for cross validation seasonal index, which we kinda recommend to avoid leakage, say 2020

    # Apply fractional differencing (3 options)
    # df_fracdiff = apply_fracDiff_FFD(df_sa, _dval, _thresh) # prado raw # 13 seconds
    # df_fracdiff = apply_fracdiff(df_sa, _dval) # uses scikit learn, faster, trust it less. less control #  1.3 seconds
    df_fracdiff = loop_dval_csv_optimized_parallel(df_sa, df_sa.columns, [_dval], _thresh, _log=False) #prado fast # 3.5-.8 seconds


# ========== plotting functions ================================================================================
    # Plot results of all tickers with bollinger bands
    # plot_fracdiff_bollinger(df_fracdiff, _lookbck, _stdv) #note: plots all fracdiffs with bollinger bands
    # plt_variants2(df, 'cl', _nmonic) # compares fracdiff's methods and raw seasonal adjustments
    # plt_variants(df, 'cl', _nmonic, _dval, _thresh) #  tests seasonal and cv's differences no fracdiff

# ========== process tickers for features in csv ===============================================================
    #note: _plt_all will plot the seasonals of all the columns together and show us the correlation matrix
    #note: _plt_1st will plot the first ticker in the list so you can see the raw, seasonal, and fracdiff data

    final_df = process_tickers(df, df_sa, df_fracdiff, tickers, _nmonic, _dval,
                               _lookbck, _stdv, _yrs_index, _plt_all=False, _plt_1st=False)
    final_df.index.name = 'date'
    output_path = fr"C:\Optima Labs Dropbox\Feature_Data\seas_adj_fracd_outputs\{_nmonic}_{_dval}_{_yrs_index}_transformed.csv"
    final_df.to_csv(output_path, index=True, index_label='date')
    print(f"Processed data saved to {output_path}")


if __name__ == "__main__":
    main()

